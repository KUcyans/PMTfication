{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import scipy.optimize as optimize\n",
    "from scipy import stats as sci\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# 2 min 22 sec in HEP04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/groups/icecube/cyan/Utils')\n",
    "from PlotUtils import setMplParam, getColour, getHistoParam \n",
    "# getHistoParam:\n",
    "# Nbins, binwidth, bins, counts, bin_centers  = \n",
    "from DB_lister import list_content, list_tables\n",
    "from ExternalFunctions import nice_string_output, add_text_to_ax\n",
    "setMplParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_event_count(conn: sql.Connection, table: str) -> int:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT COUNT(DISTINCT event_no) FROM {table}\")\n",
    "    event_count = cursor.fetchone()[0]\n",
    "    return event_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDBtoDF(file:str, table:str, N_events_total:int, N_events:int = None) -> pd.DataFrame:\n",
    "    con = sql.connect(file)\n",
    "    if N_events is None or N_events > N_events_total:\n",
    "        N_events = N_events_total\n",
    "    # Query to fetch the first `N_events` unique event_no values\n",
    "    event_no_query = f'SELECT DISTINCT event_no FROM {table} LIMIT {N_events}'\n",
    "    event_nos = pd.read_sql_query(event_no_query, con)['event_no'].tolist()\n",
    "    \n",
    "    # Use the selected event_no values to filter the main data\n",
    "    event_filter = ','.join(map(str, event_nos))  # Convert to comma-separated string for SQL IN clause\n",
    "    query = f'SELECT * FROM {table} WHERE event_no IN ({event_filter})'\n",
    "    \n",
    "    # Read data and close the connection\n",
    "    df = pd.read_sql_query(query, con)\n",
    "    con.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertDFtoDB(file:str, table:str, df: pd.DataFrame) -> None:\n",
    "    con = sql.connect(file)\n",
    "    df.to_sql(table, con, if_exists='replace', index=False)\n",
    "    con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reference_data(filepath: str) -> np.ndarray:\n",
    "    df = pd.read_csv(filepath)\n",
    "    return df.values  # Convert the DataFrame to a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addStringAndDOMtoDB(con_source: sql.Connection, \n",
    "                        source_table: str,\n",
    "                        reference_data: np.ndarray,\n",
    "                        tolerance_xy: float = 10,\n",
    "                        tolerance_z: float = 2) -> None:\n",
    "    cur_source = con_source.cursor()\n",
    "    cur_source.execute(f\"PRAGMA table_info({source_table})\")\n",
    "    existing_columns = [col[1] for col in cur_source.fetchall()]\n",
    "    \n",
    "    # Add `string` and `dom_number` columns if they don’t exist\n",
    "    if 'string' not in existing_columns:\n",
    "        cur_source.execute(f\"ALTER TABLE {source_table} ADD COLUMN string INTEGER\")\n",
    "    if 'dom_number' not in existing_columns:\n",
    "        cur_source.execute(f\"ALTER TABLE {source_table} ADD COLUMN dom_number INTEGER\")\n",
    "    \n",
    "    # Select rows where `string` or `dom_number` is NULL\n",
    "    cur_source.execute(f\"SELECT rowid, dom_x, dom_y, dom_z FROM {source_table} WHERE string IS NULL OR dom_number IS NULL\")\n",
    "    rows_to_update = cur_source.fetchall()\n",
    "    \n",
    "    # Update rows based on tolerance matching with reference data\n",
    "    for row in rows_to_update:\n",
    "        row_id, dom_x, dom_y, dom_z = row\n",
    "        \n",
    "        # Match `dom_x` and `dom_y` within specified tolerance\n",
    "        matches_xy = reference_data[\n",
    "            (np.abs(reference_data[:, 2] - dom_x) <= tolerance_xy) &\n",
    "            (np.abs(reference_data[:, 3] - dom_y) <= tolerance_xy)\n",
    "        ]\n",
    "        \n",
    "        # If any matches on x and y, proceed to check z\n",
    "        if len(matches_xy) > 0:\n",
    "            match_z = matches_xy[np.abs(matches_xy[:, 4] - dom_z) <= tolerance_z]\n",
    "            \n",
    "            if len(match_z) > 0:\n",
    "                string_val = int(match_z[0, 0])\n",
    "                dom_number_val = int(match_z[0, 1])\n",
    "                \n",
    "                # Update the row with matching `string` and `dom_number`\n",
    "                cur_source.execute(f\"UPDATE {source_table} SET string = ?, dom_number = ? WHERE rowid = ?\", (string_val, dom_number_val, row_id))\n",
    "\n",
    "    # Commit all updates\n",
    "    con_source.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTruthTableNameDB(con_source: sql.Connection) -> str:\n",
    "    cur_source = con_source.cursor()\n",
    "    cur_source.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
    "    tables = [row[0] for row in cur_source.fetchall()]\n",
    "    if 'truth' in tables:\n",
    "        truth_table = 'truth'\n",
    "    elif 'Truth' in tables:\n",
    "        truth_table = 'Truth'\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'truth' nor 'Truth' table exists in the source database.\")\n",
    "    return truth_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTruthTableDB(con_in: sql.Connection, \n",
    "                    con_out: sql.Connection, \n",
    "                    N_events: int) -> None:\n",
    "    # Get the truth table name\n",
    "    truth_table_name = getTruthTableNameDB(con_in)\n",
    "    cur_in = con_in.cursor()\n",
    "    cur_out = con_out.cursor()\n",
    "\n",
    "    # Select the first N_events unique event numbers\n",
    "    event_no_query = f\"SELECT DISTINCT event_no FROM {truth_table_name} LIMIT {N_events}\"\n",
    "    cur_in.execute(event_no_query)\n",
    "    event_nos = [row[0] for row in cur_in.fetchall()]\n",
    "\n",
    "    # Use selected event numbers to retrieve rows from truth table\n",
    "    event_filter = ','.join(map(str, event_nos))  # Convert list to comma-separated string for SQL IN clause\n",
    "    query = f\"SELECT * FROM {truth_table_name} WHERE event_no IN ({event_filter})\"\n",
    "    cur_in.execute(query)\n",
    "    rows = cur_in.fetchall()\n",
    "\n",
    "    # Copy the schema to the output connection\n",
    "    cur_in.execute(f\"PRAGMA table_info({truth_table_name})\")\n",
    "    schema_info = cur_in.fetchall()\n",
    "    create_table_query = f\"CREATE TABLE IF NOT EXISTS {truth_table_name} ({', '.join([f'{col[1]} {col[2]}' for col in schema_info])})\"\n",
    "    cur_out.execute(create_table_query)\n",
    "\n",
    "    # Insert the selected rows into the output database\n",
    "    placeholders = ', '.join(['?'] * len(schema_info))\n",
    "    insert_query = f\"INSERT INTO {truth_table_name} VALUES ({placeholders})\"\n",
    "    cur_out.executemany(insert_query, rows)\n",
    "\n",
    "    con_out.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/groups/icecube/cyan/factory/DOMification/PMTfied/test/\"\n",
    "test_source_DB_dir = test_dir + \"testSource/\"\n",
    "test_PMTfied_DB_dir = \"/groups/icecube/cyan/factory/DOMification/PMTfied/test/PMTfiedDB/\"\n",
    "test_PMTfied_parquet_dir = \"/groups/icecube/cyan/factory/DOMification/PMTfied/test/PMTfiedParquet/\"\n",
    "\n",
    "QA_DB = test_source_DB_dir + \"Level2_NuE_NuGenCCNC.022015.000110.db\"\n",
    "QA_DB_PMTfied = test_PMTfied_DB_dir + \"voici.db\"\n",
    "\n",
    "ref_str_dom_pos = \"/groups/icecube/cyan/factory/DOMification/unique_string_dom_completed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['truth', 'SRTInIcePulses', 'event_no_SRTInIcePulses', 'OnlineL2_BestFit', 'OnlineL2_SplineMPE', 'LineFit', 'GNHighestEInIceParticle', 'GNHighestEDaughter', 'MCWeightDict', 'SnowStormParameters']\n"
     ]
    }
   ],
   "source": [
    "list_tables(QA_DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTruthPA(con_source: sql.Connection, N_events: int) -> pa.Table:\n",
    "    # Get the name of the truth table\n",
    "    truth_table_name = getTruthTableNameDB(con_source)\n",
    "    cur_source = con_source.cursor()\n",
    "    \n",
    "    # Fetch the schema info for column names\n",
    "    cur_source.execute(f\"PRAGMA table_info({truth_table_name})\")\n",
    "    schema_info = cur_source.fetchall()\n",
    "    \n",
    "    # Select the first N_events unique event_no values\n",
    "    event_no_query = f\"SELECT DISTINCT event_no FROM {truth_table_name} LIMIT {N_events}\"\n",
    "    cur_source.execute(event_no_query)\n",
    "    event_nos = [row[0] for row in cur_source.fetchall()]\n",
    "\n",
    "    # Use these event numbers to filter the main query\n",
    "    event_filter = ','.join(map(str, event_nos))  # Convert to comma-separated list for SQL IN clause\n",
    "    query = f\"SELECT * FROM {truth_table_name} WHERE event_no IN ({event_filter})\"\n",
    "    cur_source.execute(query)\n",
    "    rows = cur_source.fetchall()\n",
    "    \n",
    "    # Convert the result to a dictionary compatible with PyArrow\n",
    "    truth_data_dict = {col[1]: [row[i] for row in rows] for i, col in enumerate(schema_info)}\n",
    "    truth_table_pa = pa.Table.from_pydict(truth_data_dict)\n",
    "    \n",
    "    return truth_table_pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* this conversion from db to parquet inevitably requires use of pandas dataframe.\n",
    "* it would be much desirable if the data is directly converted from I3 to PMTfied parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPMTfiedPA(con_source: sql.Connection, \n",
    "                source_table: str,\n",
    "                dom_ref_pos_file: str,\n",
    "                N_events: int) -> pa.Table:\n",
    "    dom_ref_pos = load_reference_data(dom_ref_pos_file)\n",
    "    addStringAndDOMtoDB(con_source, source_table, dom_ref_pos, N_events)\n",
    "    \n",
    "    # Select unique event numbers to use\n",
    "    cur_source = con_source.cursor()\n",
    "    event_no_query = f'SELECT DISTINCT event_no FROM {source_table} LIMIT {N_events}'\n",
    "    cur_source.execute(event_no_query)\n",
    "    event_nos = [row[0] for row in cur_source.fetchall()]\n",
    "\n",
    "    # Query rows where event_no matches the selected event numbers\n",
    "    event_filter = ','.join(map(str, event_nos))\n",
    "    query = f'SELECT * FROM {source_table} WHERE event_no IN ({event_filter})'\n",
    "    cur_source.execute(query)\n",
    "    rows = cur_source.fetchall()\n",
    "\n",
    "    # Get column names for indexing\n",
    "    columns = [description[0] for description in cur_source.description]\n",
    "    \n",
    "    event_no_idx = columns.index('event_no')\n",
    "    dom_string_idx = columns.index('string')\n",
    "    dom_number_idx = columns.index('dom_number')\n",
    "    dom_x_idx = columns.index('dom_x')\n",
    "    dom_y_idx = columns.index('dom_y')\n",
    "    dom_z_idx = columns.index('dom_z')\n",
    "    dom_time_idx = columns.index('dom_time')\n",
    "    dom_hlc_idx = columns.index('hlc')\n",
    "    dom_charge_idx = columns.index('charge')\n",
    "    pmt_area_idx = columns.index('pmt_area')\n",
    "    rde_idx = columns.index('rde')\n",
    "    saturation_status_idx = columns.index('is_saturated_dom')\n",
    "    \n",
    "    def getMaxQtotal(all_pulses_event: List[List[List[float]]]) -> float:\n",
    "        Qsums = [sum([pulse[dom_charge_idx] for pulse in pulses]) for pulses in all_pulses_event]\n",
    "        return max(Qsums)\n",
    "    \n",
    "    def getQweightedAverageDOMposition(all_pulses_event: List[List[List[float]]], maxQtotal: float) -> List[float]:\n",
    "        dom_x = [pulse[dom_x_idx] for pulses_dom in all_pulses_event for pulse in pulses_dom]\n",
    "        dom_y = [pulse[dom_y_idx] for pulses_dom in all_pulses_event for pulse in pulses_dom]\n",
    "        dom_z = [pulse[dom_z_idx] for pulses_dom in all_pulses_event for pulse in pulses_dom]\n",
    "        charge_sums = [pulse[dom_charge_idx] for pulses_dom in all_pulses_event for pulse in pulses_dom]\n",
    "\n",
    "        weighted_x = np.mean([x * charge / maxQtotal for x, charge in zip(dom_x, charge_sums)])\n",
    "        weighted_y = np.mean([y * charge / maxQtotal for y, charge in zip(dom_y, charge_sums)])\n",
    "        weighted_z = np.mean([z * charge / maxQtotal for z, charge in zip(dom_z, charge_sums)])\n",
    "\n",
    "        return [weighted_x, weighted_y, weighted_z]\n",
    "        \n",
    "    def getRelativeDOMposition(dom_x: float, dom_y: float, dom_z: float, avg_dom_position: List[float]) -> List[float]:\n",
    "        return [dom_x - avg_dom_position[0], dom_y - avg_dom_position[1], dom_z - avg_dom_position[2]]\n",
    "    \n",
    "    # NOTE pulses_dom: [pulse, ...]\n",
    "    def getDOMposition(pulses_dom: List[List[float]]) -> List[float]:\n",
    "        return [pulses_dom[0][dom_x_idx], pulses_dom[0][dom_y_idx], pulses_dom[0][dom_z_idx]]\n",
    "    \n",
    "    def getDOMstring(pulses_dom: List[List[float]]) -> int:\n",
    "        return pulses_dom[0][dom_string_idx]\n",
    "    \n",
    "    def getDOMnumber(pulses_dom: List[List[float]]) -> int:\n",
    "        return pulses_dom[0][dom_number_idx]\n",
    "    \n",
    "    def getPmtArea(pulses_dom: List[List[float]]) -> float:\n",
    "        return pulses_dom[0][pmt_area_idx]\n",
    "    \n",
    "    def getRDE(pulses_dom: List[List[float]]) -> float:\n",
    "        return pulses_dom[0][rde_idx]\n",
    "    \n",
    "    def getSaturationStatus(pulses_dom: List[List[float]]) -> int:\n",
    "        return pulses_dom[0][saturation_status_idx]\n",
    "    \n",
    "    def getFirstHlc(pulses_dom: List[List[float]]) -> List[int]:\n",
    "        n = 3\n",
    "        _fillIncomplete = -1\n",
    "        if len(pulses_dom) < n:\n",
    "            hlc = [pulse[dom_hlc_idx] for pulse in pulses_dom]\n",
    "            hlc.extend([_fillIncomplete] * (n - len(hlc)))\n",
    "        else:\n",
    "            hlc = [pulse[dom_hlc_idx] for pulse in pulses_dom[:n]]\n",
    "        return hlc\n",
    "    \n",
    "    def getFirstPulseTime(pulses_dom: List[List[float]], saturationStatus: int) -> List[float]:\n",
    "        n = 3\n",
    "        # HACK consider changing the fill values\n",
    "        _fillSaturated = -1\n",
    "        _fillIncomplete = -1\n",
    "        \n",
    "        if saturationStatus == 1:\n",
    "            pulse_times = [_fillSaturated] * n\n",
    "        elif len(pulses_dom) < n:\n",
    "            pulse_times = [pulse[dom_time_idx] for pulse in pulses_dom]\n",
    "            pulse_times.extend([_fillIncomplete] * (n - len(pulse_times)))\n",
    "        else:\n",
    "            pulse_times = [pulse[dom_time_idx] for pulse in pulses_dom[:n]]\n",
    "        return pulse_times\n",
    "    \n",
    "    # HACK necessary?\n",
    "    def getFirstHlcPulseTime(pulses_dom: List[List[float]], saturationStatus: int) -> List[float]:\n",
    "        n = 3\n",
    "        _fillSaturated = -1\n",
    "        _fillIncomplete = -1\n",
    "        if saturationStatus == 1:\n",
    "            pulse_times = [_fillSaturated] * n\n",
    "        elif len(pulses_dom) < n:\n",
    "            pulse_times = [pulse[dom_time_idx] for pulse in pulses_dom if pulse[dom_hlc_idx] == 1]\n",
    "            pulse_times.extend([_fillIncomplete] * (n - len(pulse_times)))\n",
    "        else:\n",
    "            pulse_times = [pulse[dom_time_idx] for pulse in pulses_dom if pulse[dom_hlc_idx] == 1][:n]\n",
    "        return pulse_times\n",
    "        \n",
    "    def getElapsedTimeUntilChargeFraction(pulses_dom: List[List[float]], saturationStatus: int, percentile1 = 10, percentile2 = 50) -> List[float]:\n",
    "        # HACK consider changing the fill values\n",
    "        _fillSaturated = -1\n",
    "        _fillIncomplete = -1\n",
    "        if saturationStatus == 1:\n",
    "            times = [_fillSaturated] * 2\n",
    "        elif len(pulses_dom) < 2:\n",
    "            times = [_fillIncomplete] * 2\n",
    "        else:\n",
    "            Qtotal = sum([pulse[dom_charge_idx] for pulse in pulses_dom])\n",
    "            t_0 = pulses_dom[0][dom_time_idx]\n",
    "            Qcum = 0\n",
    "            T_first, T_second = -1, -1 # if these are not -1, then they are assigned\n",
    "            for pulse in pulses_dom:\n",
    "                Qcum += pulse[dom_charge_idx]\n",
    "                if Qcum > percentile1 / 100 * Qtotal and T_first == -1:\n",
    "                    T_first = pulse[dom_time_idx] - t_0\n",
    "                if Qcum > percentile2 / 100 * Qtotal:\n",
    "                    T_second = pulse[dom_time_idx] - t_0\n",
    "                    break\n",
    "            times = [T_first, T_second]\n",
    "        return times\n",
    "    \n",
    "    def getStandardDeviation(pulse_times: List[float], saturationStatus: int) -> float:\n",
    "        # HACK consider changing the fill values\n",
    "        _fillSaturated = 0\n",
    "        _fillIncomplete = 0\n",
    "        if saturationStatus == 1:\n",
    "            sigmaT = _fillSaturated\n",
    "        elif len(pulse_times) < 2:\n",
    "            sigmaT = _fillIncomplete\n",
    "        else:\n",
    "            sigmaT = np.std(pulse_times)\n",
    "        return sigmaT\n",
    "    \n",
    "    def getFirstChargeReadout(pulses: List[List[float]], saturationStatus: int) -> List[float]:\n",
    "        # HACK consider changing the fill values\n",
    "        _fillSaturated = -1\n",
    "        _fillIncomplete = -1\n",
    "        n = 3\n",
    "        if saturationStatus == 1:\n",
    "            charge_readouts = [_fillSaturated] * n\n",
    "        elif len(pulses) < n:\n",
    "            charge_readouts = [pulse[dom_charge_idx] for pulse in pulses]\n",
    "            charge_readouts.extend([_fillIncomplete] * (n - len(charge_readouts)))\n",
    "        else:\n",
    "            charge_readouts = [pulse[dom_charge_idx] for pulse in pulses[:n]]\n",
    "        return charge_readouts\n",
    "    \n",
    "    def getAccumulatedChargeAfterNanoSec(pulses: List[List[float]], saturationStatus: int, interval1 = 25, interval2 = 75) -> List[float]:\n",
    "        # HACK consider changing the fill values\n",
    "        _fillSaturated = -1\n",
    "        _fillIncomplete = -1\n",
    "        if saturationStatus == 1:\n",
    "            Qs = [_fillSaturated] * 3\n",
    "        elif len(pulses) < 1:\n",
    "            Qs = [_fillIncomplete] * 3\n",
    "        else:\n",
    "            Qtotal = sum([pulse[dom_charge_idx] for pulse in pulses])\n",
    "            t_0 = pulses[0][dom_time_idx]\n",
    "            Qinterval1 = sum([pulse[dom_charge_idx] for pulse in pulses if pulse[dom_time_idx] - t_0 < interval1])\n",
    "            Qinterval2 = sum([pulse[dom_charge_idx] for pulse in pulses if pulse[dom_time_idx] - t_0 < interval2])\n",
    "            Qs = [Qinterval1, Qinterval2, Qtotal]\n",
    "        return Qs\n",
    "    \n",
    "    def processDOM(pulses: List[List[float]], avg_dom_position: List[float]):\n",
    "        dom_string = getDOMstring(pulses)\n",
    "        dom_number = getDOMnumber(pulses)\n",
    "        dom_x, dom_y, dom_z = getDOMposition(pulses)\n",
    "        dom_x_rel, dom_y_rel, dom_z_rel = getRelativeDOMposition(dom_x, dom_y, dom_z, avg_dom_position)\n",
    "        pmt_area = getPmtArea(pulses)\n",
    "        rde = getRDE(pulses)\n",
    "        saturation_status = getSaturationStatus(pulses)\n",
    "        \n",
    "        # Get remaining features\n",
    "        first_three_charge_readout = getFirstChargeReadout(pulses, saturation_status)\n",
    "        accumulated_charge_after_nano_sec = getAccumulatedChargeAfterNanoSec(pulses, saturation_status)\n",
    "        first_three_pulse_time = getFirstPulseTime(pulses, saturation_status)\n",
    "        # first_three_hlc_pulse_time = getFirstHlcPulseTime(pulses, saturation_status)\n",
    "        first_three_hlc = getFirstHlc(pulses)\n",
    "        elapsed_time_until_charge_fraction = getElapsedTimeUntilChargeFraction(pulses, saturation_status)\n",
    "        standard_deviation = getStandardDeviation([pulse[dom_time_idx] for pulse in pulses], saturation_status)\n",
    "        \n",
    "        data_dom = ([dom_string, dom_number]            # dom_number\n",
    "                    + [dom_x, dom_y, dom_z]             # dom_x, dom_y, dom_z\n",
    "                    + [dom_x_rel, dom_y_rel, dom_z_rel] # dom_x_rel, dom_y_rel, dom_z_rel\n",
    "                    + [pmt_area, rde, saturation_status]# pmt_area, rde, saturationStatus\n",
    "                    + first_three_charge_readout        # q1, q2, q3\n",
    "                    + accumulated_charge_after_nano_sec # Q25, Q75, Qtotal\n",
    "                    + first_three_hlc                   # hlc1, hlc2, hlc3\n",
    "                    + first_three_pulse_time            # t1, t2, t3\n",
    "                    # + first_three_hlc_pulse_time        # t1_hlc, t2_hlc, t3_hlc\n",
    "                    + elapsed_time_until_charge_fraction# T10, T50\n",
    "                    + [standard_deviation]              # sigmaT\n",
    "                    )\n",
    "        return data_dom            \n",
    "    # original data\n",
    "    events_doms_pulses = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
    "    # new data\n",
    "    processed_data = []\n",
    "    for row in rows:\n",
    "        event_no = row[event_no_idx]\n",
    "        string = row[dom_string_idx]\n",
    "        dom_number = row[dom_number_idx]\n",
    "        events_doms_pulses[event_no][string][dom_number].append(row)\n",
    "    \n",
    "    # NOTE data structure\n",
    "    # events_doms_pulses  : {event_no: {string: {dom_number: [pulse, ...], ...}, ...}, ...}\n",
    "    # strings_doms_pulses :            {string: {dom_number: [pulse, ...], ...}, ...}\n",
    "    # doms_pulses         :                     {dom_number: [pulse, ...], ...}\n",
    "    # pulses              :                                  [pulse, ...]\n",
    "    for event_no, strings_doms_pulses in events_doms_pulses.items():\n",
    "        for doms_pulses in strings_doms_pulses.values():\n",
    "            # Convert the values to a list of pulses (rows)\n",
    "            all_pulses_event = list(doms_pulses.values())\n",
    "            maxQtotal = getMaxQtotal(all_pulses_event)\n",
    "            avg_dom_position = getQweightedAverageDOMposition(all_pulses_event, maxQtotal)\n",
    "            for pulses in doms_pulses.values():\n",
    "                dom_data = [event_no] + processDOM(pulses, avg_dom_position)\n",
    "                processed_data.append(dom_data)\n",
    "\n",
    "    # Convert the processed data into a DataFrame for easier handling\n",
    "    df_processed = pd.DataFrame(processed_data, columns=[\n",
    "        'event_no', 'dom_string', 'dom_number', # indices\n",
    "        'dom_x', 'dom_y', 'dom_z',  \n",
    "        'dom_x_rel', 'dom_y_rel', 'dom_z_rel', \n",
    "        'pmt_area', 'rde', 'saturation_status', \n",
    "        'q1', 'q2', 'q3', \n",
    "        'Q25', 'Q75', 'Qtotal',\n",
    "        'hlc1', 'hlc2', 'hlc3', \n",
    "        't1', 't2', 't3', \n",
    "        'T10', 'T50', 'sigmaT'\n",
    "    ])\n",
    "    pa_processed = pa.Table.from_pandas(df_processed)\n",
    "    return pa_processed  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `runPMTfication_DB_Parquet` layer is intended to intervene the process of sqlite connection and close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runPMTfication_DB_Parquet(source_file: str, \n",
    "                            source_table: str, \n",
    "                            dom_ref_pos_file: str,\n",
    "                            N_events: int = None) -> (pa.Table, pa.Table):\n",
    "    con_source = None\n",
    "    try:\n",
    "        base_name = os.path.splitext(os.path.basename(source_file))[0]\n",
    "        \n",
    "        con_source = sql.connect(source_file)\n",
    "        N_events_total = get_table_event_count(con_source, source_table)\n",
    "        \n",
    "        # Determine N_events if it is not provided or exceeds N_events_total\n",
    "        if N_events is None or N_events > N_events_total:\n",
    "            N_events = N_events_total\n",
    "        \n",
    "        # Get the PyArrow tables for PMTfied and truth data based on N_events\n",
    "        pa_pmtfied = getPMTfiedPA(con_source, source_table, dom_ref_pos_file, N_events)\n",
    "        pa_truth = getTruthPA(con_source, N_events)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the PMTfication: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    finally:\n",
    "        if con_source:\n",
    "            con_source.close()\n",
    "\n",
    "    return pa_pmtfied, pa_truth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeParquet(source_file: str,\n",
    "                source_table: str,\n",
    "                dom_ref_pos_file: str,\n",
    "                N_events: int = None,\n",
    "                pmtfied_file: str = None,\n",
    "                truth_file: str = None) -> None:\n",
    "    pa_pmtfied, pa_truth = runPMTfication_DB_Parquet(source_file, source_table, dom_ref_pos_file, N_events)\n",
    "    if pmtfied_file is None:\n",
    "        pmtfied_file = f\"{source_file}_PMTfied.parquet\"\n",
    "    pq.write_table(pa_pmtfied, pmtfied_file)\n",
    "    pq.write_table(pa_truth, truth_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "test_dir = \"/groups/icecube/cyan/factory/DOMification/PMTfied/test/\"\n",
    "test_source_DB_dir = test_dir + \"testSource/\"\n",
    "test_PMTfied_DB_dir = \"/groups/icecube/cyan/factory/DOMification/PMTfied/test/PMTfiedDB/\"\n",
    "test_PMTfied_parquet_dir = \"/groups/icecube/cyan/factory/DOMification/PMTfied/test/PMTfiedParquet/\"\n",
    "\n",
    "QA_DB = test_source_DB_dir + \"Level2_NuE_NuGenCCNC.022015.000110.db\"\n",
    "QA_DB_PMTfied = test_PMTfied_DB_dir + \"voila.db\"\n",
    "\n",
    "ref_str_dom_pos = \"/groups/icecube/cyan/factory/DOMification/unique_string_dom_completed.csv\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table_event_count(file: str, table: str):\n",
    "    conn = sql.connect(file)\n",
    "    event_count = get_table_event_count(conn, table)\n",
    "    print(f\"Table {table} has {event_count} unique events\")\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table SRTInIcePulses has 22 unique events\n"
     ]
    }
   ],
   "source": [
    "print_table_event_count(QA_DB, \"SRTInIcePulses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeParquet(source_file=QA_DB,\n",
    "            source_table=\"SRTInIcePulses\",\n",
    "            dom_ref_pos_file=ref_str_dom_pos,\n",
    "            N_events=20,\n",
    "            pmtfied_file=\"test_eventwise_pmtfied_features.parquet\",\n",
    "            truth_file=\"test_eventwise_truth.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeThisFeatureParquet = \"/groups/icecube/cyan/factory/DOMification/test_eventwise_pmtfied_features.parquet\"\n",
    "seeThisTruthParquet = \"/groups/icecube/cyan/factory/DOMification/test_eventwise_truth.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeThisFeature_df = pq.read_table(seeThisFeatureParquet).to_pandas()\n",
    "seeThisTruth_df = pq.read_table(seeThisTruthParquet).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N events of feature: 20\n",
      "N events of truth: 20\n"
     ]
    }
   ],
   "source": [
    "print(f\"N events of feature: {seeThisFeature_df['event_no'].nunique()}\")\n",
    "print(f\"N events of truth: {seeThisTruth_df['event_no'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_PMTfication_DB_parquet(source_DB_dir: str, target_dir: str, N_events: int = None) -> None:\n",
    "    db_files = [f for f in os.listdir(source_DB_dir) if f.endswith('.db')]\n",
    "    \n",
    "    pmtfied_tables = []\n",
    "    truth_tables = []\n",
    "    \n",
    "    for db_file in tqdm(db_files, desc=\"Processing...\"):\n",
    "        source_DB = os.path.join(source_DB_dir, db_file)\n",
    "        \n",
    "        # Connect to each database to get N_events_total\n",
    "        with sql.connect(source_DB) as con:\n",
    "            cur = con.cursor()\n",
    "            cur.execute(\"SELECT COUNT(DISTINCT event_no) FROM SRTInIcePulses\")\n",
    "            N_events_total = cur.fetchone()[0]\n",
    "        \n",
    "        # Run PMTfication for each database file with N_events_total and optional N_events\n",
    "        pa_pmtfied, pa_truth = runPMTfication_DB_Parquet(\n",
    "            source_file=source_DB, \n",
    "            source_table=\"SRTInIcePulses\",\n",
    "            dom_ref_pos_file=ref_str_dom_pos,\n",
    "            N_events_total=N_events_total,\n",
    "            N_events=N_events\n",
    "        )\n",
    "        \n",
    "        pmtfied_tables.append(pa_pmtfied)\n",
    "        truth_tables.append(pa_truth)\n",
    "    \n",
    "    # Combine all PMTfied tables and write to a single Parquet file\n",
    "    combined_pmtfied_table = pa.concat_tables(pmtfied_tables)\n",
    "    combined_pmtfied_file = os.path.join(target_dir, \"combined_PMTfied.parquet\")\n",
    "    pq.write_table(combined_pmtfied_table, combined_pmtfied_file)\n",
    "\n",
    "    # Combine all truth tables and write to a single Parquet file\n",
    "    combined_truth_table = pa.concat_tables(truth_tables)\n",
    "    combined_truth_file = os.path.join(target_dir, \"combined_truth.parquet\")\n",
    "    pq.write_table(combined_truth_table, combined_truth_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...: 100%|██████████| 30/30 [05:38<00:00, 11.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# batch_PMTfication_DB_parquet(test_source_DB_dir, test_PMTfied_parquet_dir)\n",
    "# individual: 1 min 58 sec\n",
    "# combining: 5 min 40 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeThis = \"/groups/icecube/cyan/factory/DOMification/PMTfied/test/testSource/PMTfiedParquet/event_no=0/276c6d50570445a984e54548bb4dcbe5-0.parquet\"\n",
    "seeThat = \"/groups/icecube/cyan/factory/DOMification/PMTfied/test/testSource/PMTfiedParquet/event_no=0/c0e0546be44940d9bd6dc39391a627b8-0.parquet\"\n",
    "compareThis = \"/groups/icecube/cyan/factory/DOMification/PMTfied/test/PMTfiedDB/Level2_NuE_NuGenCCNC.022015.000110.db\"\n",
    "# df_seeThis = pq.read_table(seeThis).to_pandas()\n",
    "# df_seeThat = pq.read_table(seeThat).to_pandas()\n",
    "df_compareThis = convertDBtoDF(compareThis, \"PMTsummarised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['energy', 'position_x', 'position_y', 'position_z', 'azimuth', 'zenith',\n",
       "       'pid', 'event_time', 'interaction_type', 'elasticity', 'RunID',\n",
       "       'SubrunID', 'EventID', 'SubEventID', 'dbang_decay_length',\n",
       "       'track_length', 'stopped_muon', 'energy_track', 'energy_cascade',\n",
       "       'inelasticity', 'DeepCoreFilter_13', 'CascadeFilter_13',\n",
       "       'MuonFilter_13', 'OnlineL2Filter_17', 'L3_oscNext_bool',\n",
       "       'L4_oscNext_bool', 'L5_oscNext_bool', 'L6_oscNext_bool',\n",
       "       'L7_oscNext_bool', 'Homogenized_QTot', 'MCLabelClassification',\n",
       "       'MCLabelCoincidentMuons', 'MCLabelBgMuonMCPE',\n",
       "       'MCLabelBgMuonMCPECharge', 'GNLabelTrackEnergyDeposited',\n",
       "       'GNLabelTrackEnergyOnEntrance', 'GNLabelTrackEnergyOnEntrancePrimary',\n",
       "       'GNLabelTrackEnergyDepositedPrimary', 'GNLabelEnergyPrimary',\n",
       "       'GNLabelCascadeEnergyDepositedPrimary', 'GNLabelCascadeEnergyDeposited',\n",
       "       'GNLabelEnergyDepositedTotal', 'GNLabelEnergyDepositedPrimary',\n",
       "       'GNHighestEInIceParticle', 'GNLabelHighestEInIceParticleIsChild',\n",
       "       'GNLabelHighestEInIceParticleDistance',\n",
       "       'GNLabelHighestEInIceParticleEFraction',\n",
       "       'GNLabelHighestEDaughterDistance', 'GNLabelHighestEDaughterEFraction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seeThis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dom_string', 'dom_number', 'dom_x', 'dom_y', 'dom_z', 'dom_x_rel',\n",
       "       'dom_y_rel', 'dom_z_rel', 'pmt_area', 'rde', 'saturation_status', 'q1',\n",
       "       'q2', 'q3', 'Q25', 'Q75', 'Qtotal', 'hlc1', 'hlc2', 'hlc3', 't1', 't2',\n",
       "       't3', 'T10', 'T50', 'sigmaT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seeThat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['event_no', 'dom_string', 'dom_number', 'dom_x', 'dom_y', 'dom_z',\n",
       "       'dom_x_rel', 'dom_y_rel', 'dom_z_rel', 'pmt_area', 'rde',\n",
       "       'saturation_status', 'q1', 'q2', 'q3', 'Q25', 'Q75', 'Qtotal', 'hlc1',\n",
       "       'hlc2', 'hlc3', 't1', 't2', 't3', 'T10', 'T50', 'sigmaT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_compareThis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchPMTficationProcessor:\n",
    "    def __init__(self, source_dir, target_dir, dom_ref_pos_file, events_per_file):\n",
    "        self.source_dir = source_dir\n",
    "        self.target_dir = target_dir\n",
    "        self.dom_ref_pos_file = dom_ref_pos_file\n",
    "        self.events_per_file = events_per_file\n",
    "\n",
    "    def shift_event_no(self, event_no: int, subdirectory: int, shift_bits: int =24) -> int:\n",
    "        return (subdirectory << shift_bits) | event_no\n",
    "\n",
    "    def generate_receipt(self, original_file, event_mappings, target_file):\n",
    "        \"\"\"Creates a receipt summarising the processed data.\"\"\"\n",
    "        receipt_df = pd.DataFrame(event_mappings, columns=['original_event_no', 'event_no'])\n",
    "        receipt_df['source_file'] = original_file\n",
    "        receipt_path = target_file.replace('.parquet', '_receipt.csv')\n",
    "        receipt_df.to_csv(receipt_path, index=False)\n",
    "\n",
    "    def process_file_in_batches(self, db_file, subdirectory):\n",
    "        source_file = os.path.join(self.source_dir, db_file)\n",
    "        target_subdir = os.path.join(self.target_dir, os.path.dirname(db_file))\n",
    "        os.makedirs(target_subdir, exist_ok=True)\n",
    "        \n",
    "        # Open the source DB file\n",
    "        con_source = sql.connect(source_file)\n",
    "        N_events_total = get_table_event_count(con_source, \"SRTInIcePulses\")\n",
    "\n",
    "        event_offset = 0\n",
    "        for i in range(0, N_events_total, self.events_per_file):\n",
    "            N_events = min(self.events_per_file, N_events_total - i)\n",
    "            event_mappings = []\n",
    "            \n",
    "            # Generate target file name\n",
    "            target_file = os.path.join(target_subdir, f\"{db_file}_{i//self.events_per_file}.parquet\")\n",
    "            \n",
    "            # Fetch and process data\n",
    "            pa_pmtfied, pa_truth = runPMTfication_DB_Parquet(source_file, \"SRTInIcePulses\", self.dom_ref_pos_file, N_events)\n",
    "            \n",
    "            # Shift event numbers and log original-to-new mappings\n",
    "            for idx in range(len(pa_pmtfied.column('event_no'))):\n",
    "                original_event_no = pa_pmtfied.column('event_no')[idx].as_py()\n",
    "                new_event_no = self.shift_event_no(original_event_no, subdirectory)\n",
    "                event_mappings.append({'original_event_no': original_event_no, 'event_no': new_event_no})\n",
    "                pa_pmtfied = pa_pmtfied.set_column(\n",
    "                    pa_pmtfied.schema.get_field_index('event_no'), 'event_no', pa.array([new_event_no])\n",
    "                )\n",
    "            \n",
    "            # Save processed data and generate receipt\n",
    "            pq.write_table(pa_pmtfied, target_file)\n",
    "            self.generate_receipt(db_file, event_mappings, target_file)\n",
    "            \n",
    "            event_offset += N_events\n",
    "\n",
    "        con_source.close()\n",
    "\n",
    "    def process_all_files(self):\n",
    "        db_files = []\n",
    "        for root, _, files in os.walk(self.source_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.db'):\n",
    "                    db_files.append(os.path.relpath(os.path.join(root, file), self.source_dir))\n",
    "\n",
    "        for subdirectory, db_file in enumerate(tqdm(db_files, desc=\"Processing database files\")):\n",
    "            self.process_file_in_batches(db_file, subdirectory)\n",
    "\n",
    "# Usage\n",
    "processor = BatchPMTficationProcessor(\n",
    "    source_dir='/path/to/source',\n",
    "    target_dir='/path/to/target',\n",
    "    dom_ref_pos_file='/path/to/ref_str_dom_pos',\n",
    "    events_per_file=2000\n",
    ")\n",
    "processor.process_all_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
